{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1n7lNWZ72gNMlytTv6C8jsfDDQpfuI8vS","authorship_tag":"ABX9TyPHLC63vA+dMiljyfz9jx1Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install nltk spacy\n","!python -m spacy download en_core_web_sm\n"],"metadata":{"id":"Muy-7nJxnn4U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745502751174,"user_tz":-60,"elapsed":21402,"user":{"displayName":"tom compton","userId":"11608738456980900557"}},"outputId":"5b29f681-4c1a-47aa-f961-8d4a68744e5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n","Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n","Collecting en-core-web-sm==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","import pickle\n","\n","# Detect if running in Google Colab\n","IN_COLAB = 'google.colab' in sys.modules\n","\n","if IN_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    base_dir = \"/content/drive/MyDrive/Smiles Discourse Analysis\"\n","else:\n","    base_dir = \"path/to/your/local/project/folder\" # add directory if running locally\n","# Define save directory\n","pickle_dir = os.path.join(base_dir, \"pickles\")\n","os.makedirs(pickle_dir, exist_ok=True)\n","output_dir = os.path.join(base_dir, \"output\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lf0sig_S-cGg","executionInfo":{"status":"ok","timestamp":1745504359441,"user_tz":-60,"elapsed":992,"user":{"displayName":"tom compton","userId":"11608738456980900557"}},"outputId":"2b0086b8-6581-47e9-ce7d-e2e0c2c09439"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["with open(os.path.join(pickle_dir, 'full_self_help.pkl'), 'rb') as f:\n","   self_help_fulltext = pickle.load(f)\n","\n","with open(os.path.join(pickle_dir, 'full_thrift.pkl'), 'rb') as f:\n","    thrift_fulltext= pickle.load(f)\n"],"metadata":{"id":"F8mfqv0pnrhw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","stop_words = set(stopwords.words('english'))"],"metadata":{"id":"EiJI-XOzcXnr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745501097649,"user_tz":-60,"elapsed":9500,"user":{"displayName":"tom compton","userId":"11608738456980900557"}},"outputId":"01c116c4-6382-427c-a70f-1fc2d334864f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["import re\n","\n","def clean_text(text):\n","    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # remove punctuation (.,?! etc.)\n","    text = re.sub(r\"\\d+\", \"\", text)      # remove digits\n","    text = re.sub(r\"\\s+\", \" \", text)     # collapse multiple spaces\n","    return text.strip().lower()"],"metadata":{"id":"sOMUCQjT_I2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["self_help_fulltext = clean_text(self_help_fulltext)\n","thrift_fulltext = clean_text(thrift_fulltext)"],"metadata":{"id":"VlXzI6Xe_KhS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sh_tokens = word_tokenize(self_help_fulltext)\n","thrift_tokens = word_tokenize(thrift_fulltext)\n","sh_tokens = [w for w in sh_tokens if not w in stop_words]\n","thrift_tokens = [w for w in thrift_tokens if not w in stop_words]"],"metadata":{"id":"V_e1XVwRhpuY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","import pandas as pd\n","# Notice a problem with the list? It has 'men' and 'man' this could be resolved by using a lemmatiser\n","# Also other stopword lists could be used together to further reduce less valuable terms\n","# Stopwords are a tricky issue because some lists may remove useful words, so if you are unsure search the list for the term then remove it if necessary\n","sh_word_freq = Counter(sh_tokens)\n","thrift_word_freq = Counter(thrift_tokens)\n","top_sh_words = sh_word_freq.most_common(20)\n","top_thrift_words = thrift_word_freq.most_common(20)\n","print('Top Self-Help Words: ')\n","print(top_sh_words)\n","print('Top Thrift Words: ')\n","print(top_thrift_words)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"74RiWAVhjQhT","executionInfo":{"status":"ok","timestamp":1745505989491,"user_tz":-60,"elapsed":49,"user":{"displayName":"tom compton","userId":"11608738456980900557"}},"outputId":"24c68d20-0b7e-4228-b612-2af70366a8b3"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["Top Self-Help Words: \n","[('one', 458), ('life', 386), ('man', 384), ('great', 346), ('time', 331), ('men', 322), ('said', 302), ('upon', 297), ('work', 275), ('may', 247), ('years', 233), ('many', 225), ('would', 210), ('first', 201), ('character', 197), ('made', 189), ('found', 181), ('much', 172), ('could', 171), ('industry', 169)]\n","Top Thrift Words: \n","[('man', 409), ('one', 379), ('men', 372), ('may', 281), ('life', 277), ('money', 276), ('upon', 236), ('would', 232), ('much', 229), ('savings', 225), ('time', 223), ('many', 221), ('good', 213), ('mr', 209), ('great', 206), ('must', 185), ('said', 182), ('means', 179), ('people', 178), ('working', 177)]\n"]}]},{"cell_type":"code","source":["with open(os.path.join(pickle_dir, 'self_help.pkl'), 'rb') as f:\n","    self_help_sentences = pickle.load(f)\n","\n","with open(os.path.join(pickle_dir, 'thrift.pkl'), 'rb') as f:\n","    thrift_sentences = pickle.load(f)"],"metadata":{"id":"Us9ufUY-HlSf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","# So why didn't we begin with spacy? If you run the cell, you will see.\n","# The time for spacy is much longer, but the spacy lemmas will be most effective and not leave errors such as keeping 'men' and 'man' separate\n","nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n","sh_lemmas = []\n","th_lemmas = []\n","for sentence in self_help_sentences:\n","  for doc in nlp.pipe(sentence, batch_size=1000):\n","    lemmas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n","    sh_lemmas.append(lemmas)\n","for sentence in thift_sentences:\n","  for doc in nlp.pipe(thrift_fulltext, batch_size=1000):\n","    lemmas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n","    th_lemmas.append(lemmas)\n","\n"],"metadata":{"id":"ZfArnOuJEllb","executionInfo":{"status":"ok","timestamp":1745505201459,"user_tz":-60,"elapsed":379154,"user":{"displayName":"tom compton","userId":"11608738456980900557"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["# Lemmatise + remove punctuation, spaces, and stopwords\n","sh_lemmas = []\n","th_lemmas = []\n","\n","# Self-Help\n","for doc in nlp.pipe(self_help_sentences, batch_size=1000):\n","    lemmas = [\n","        token.lemma_ for token in doc\n","        if not token.is_punct and not token.is_space and token.lemma_.lower() not in stop_words and len(token.lemma_) >2\n","    ]\n","    sh_lemmas.append(lemmas)\n","\n","# Thrift\n","for doc in nlp.pipe(thrift_sentences, batch_size=1000):\n","    lemmas = [\n","        token.lemma_ for token in doc\n","        if not token.is_punct and not token.is_space and token.lemma_.lower() not in stop_words and len(token.lemma_) >2\n","    ]\n","    th_lemmas.append(lemmas)\n"],"metadata":{"id":"pbKo1J_cOZFW","executionInfo":{"status":"ok","timestamp":1745505951480,"user_tz":-60,"elapsed":27972,"user":{"displayName":"tom compton","userId":"11608738456980900557"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["# If you want to, you can load the lemmas into tokens for the rest of this notebook\n","# This will depend on the goals of the research\n","flat_sh_lemmas = [lemma for sent in sh_lemmas for lemma in sent]\n","flat_th_lemmas = [lemma for sent in th_lemmas for lemma in sent]\n","\n","# Now this works perfectly\n","sh_word_freq = Counter(flat_sh_lemmas)\n","thrift_word_freq = Counter(flat_th_lemmas)\n","\n","top_sh_words = sh_word_freq.most_common(20)\n","top_thrift_words = thrift_word_freq.most_common(20)\n","\n","print('Top Self-Help Words: ')\n","print(top_sh_words)\n","print('Top Thrift Words: ')\n","print(top_thrift_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gd1YMhtzP6_b","executionInfo":{"status":"ok","timestamp":1745505951513,"user_tz":-60,"elapsed":16,"user":{"displayName":"tom compton","userId":"11608738456980900557"}},"outputId":"3e141ffc-93fd-40e5-be57-03a23163c732"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["Top Self-Help Words: \n","[('man', 781), ('work', 547), ('one', 493), ('great', 460), ('say', 431), ('life', 429), ('make', 393), ('time', 382), ('upon', 299), ('year', 285), ('find', 273), ('good', 272), ('well', 259), ('may', 246), ('many', 225), ('character', 215), ('become', 215), ('labour', 214), ('would', 210), ('first', 209)]\n","Top Thrift Words: \n","[('man', 848), ('one', 421), ('work', 404), ('say', 360), ('make', 356), ('well', 329), ('good', 319), ('money', 306), ('life', 295), ('time', 286), ('may', 282), ('great', 276), ('year', 268), ('upon', 238), ('would', 236), ('much', 230), ('many', 221), ('become', 216), ('class', 214), ('Mr.', 213)]\n"]}]},{"cell_type":"code","source":["# Subtract frequencies to get unique words in each corpus\n","unique_to_sh = sh_word_freq - thrift_word_freq\n","unique_to_thrift = thrift_word_freq - sh_word_freq\n","\n","# This provides a crude way of looking at the differences between the corpora.\n","# However, this is only useful when comparing the this output to the most common words prior to subtraction\n","print(f\"\\nüìò Top 20 words that appear in the *Self-Help* corpus but not (or less often) in *Thrift* ‚Äî based on frequency counts after subtracting shared words:\")\n","print(unique_to_sh.most_common(20))\n","\n","print(f\"\\nüìó Top 20 words that appear in the *Thrift* corpus but not (or less often) in *Self-Help* ‚Äî again, based on frequency counts after subtraction:\")\n","print(unique_to_thrift.most_common(20))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ead_9oLzApY8","executionInfo":{"status":"ok","timestamp":1745505964940,"user_tz":-60,"elapsed":46,"user":{"displayName":"tom compton","userId":"11608738456980900557"}},"outputId":"82039d5e-e2ec-42db-e1fd-3a8186a82cd5"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üìò Top 20 words that appear in the *Self-Help* corpus but not (or less often) in *Thrift* ‚Äî based on frequency counts after subtracting shared words:\n","[('great', 184), ('study', 168), ('character', 157), ('work', 143), ('life', 134), ('find', 127), ('boy', 106), ('success', 104), ('Sir', 98), ('time', 96), ('mind', 96), ('industry', 90), ('learn', 88), ('first', 88), ('machine', 84), ('career', 75), ('one', 72), ('succeed', 72), ('though', 72), ('say', 71)]\n","\n","üìó Top 20 words that appear in the *Thrift* corpus but not (or less often) in *Self-Help* ‚Äî again, based on frequency counts after subtraction:\n","[('money', 230), ('saving', 166), ('debt', 154), ('class', 145), ('save', 140), ('bank', 134), ('pound', 134), ('pay', 128), ('wage', 126), ('society', 123), ('Mr.', 120), ('people', 119), ('spend', 110), ('home', 100), ('hundred', 99), ('workman', 98), ('poor', 93), ('woman', 93), ('house', 89), ('provide', 81)]\n"]}]},{"cell_type":"code","source":["# Remember to base comparisons off percentages as each dataset has different word amounts\n","# Expect the highest frequencies to still be quite low, using lemmas will increase them\n","df_top_sh = pd.DataFrame(top_sh_words,columns=['word','freq'])\n","df_top_th = pd.DataFrame(top_thrift_words,columns=['word','freq'])\n","df_top_sh['percentage'] = round(df_top_sh['freq'] / df_top_sh['freq'].sum(),2)\n","df_top_th['percentage'] = round(df_top_th['freq'] / df_top_th['freq'].sum(),2)\n","print(f'self-help top words: ')\n","print(df_top_sh)\n","output_dir = os.path.join(base_dir, \"output\")\n","df_top_sh.to_csv(os.path.join(output_dir, 'self_help_top_words.csv'), index=False)\n","print(f'thrift top words: ')\n","print(df_top_th)\n","df_top_th.to_csv(os.path.join(output_dir, 'thrift_top_words.csv'), index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sKArsRTD7jXG","executionInfo":{"status":"ok","timestamp":1745506105703,"user_tz":-60,"elapsed":36,"user":{"displayName":"tom compton","userId":"11608738456980900557"}},"outputId":"4537f76c-b13d-43bd-d93e-cbfde6218524"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["self-help top words: \n","         word  freq  percentage\n","0         one   458        0.09\n","1        life   386        0.07\n","2         man   384        0.07\n","3       great   346        0.07\n","4        time   331        0.06\n","5         men   322        0.06\n","6        said   302        0.06\n","7        upon   297        0.06\n","8        work   275        0.05\n","9         may   247        0.05\n","10      years   233        0.04\n","11       many   225        0.04\n","12      would   210        0.04\n","13      first   201        0.04\n","14  character   197        0.04\n","15       made   189        0.04\n","16      found   181        0.03\n","17       much   172        0.03\n","18      could   171        0.03\n","19   industry   169        0.03\n","thrift top words: \n","       word  freq  percentage\n","0       man   409        0.08\n","1       one   379        0.08\n","2       men   372        0.08\n","3       may   281        0.06\n","4      life   277        0.06\n","5     money   276        0.06\n","6      upon   236        0.05\n","7     would   232        0.05\n","8      much   229        0.05\n","9   savings   225        0.05\n","10     time   223        0.05\n","11     many   221        0.05\n","12     good   213        0.04\n","13       mr   209        0.04\n","14    great   206        0.04\n","15     must   185        0.04\n","16     said   182        0.04\n","17    means   179        0.04\n","18   people   178        0.04\n","19  working   177        0.04\n"]}]},{"cell_type":"code","source":["sh_bigrams = nltk.FreqDist(nltk.bigrams(sh_tokens))\n","thrift_bigrams = nltk.FreqDist(nltk.bigrams(thrift_tokens))\n","print(sh_bigrams.most_common(20))\n","print(thrift_bigrams.most_common(20))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hFURVyTmjUiD","executionInfo":{"status":"ok","timestamp":1745501851094,"user_tz":-60,"elapsed":184,"user":{"displayName":"tom compton","userId":"11608738456980900557"}},"outputId":"44c47e2a-4ffb-4f52-db2f-85672adb7a1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(('one', 'day'), 31), (('many', 'years'), 24), (('young', 'man'), 23), (('one', 'occasion'), 22), (('sir', 'john'), 22), (('several', 'years'), 21), (('set', 'work'), 17), (('young', 'men'), 16), (('robert', 'peel'), 16), (('early', 'life'), 15), (('indefatigable', 'industry'), 14), (('ten', 'years'), 14), (('men', 'business'), 13), (('years', 'old'), 13), (('like', 'manner'), 13), (('years', 'age'), 13), (('called', 'upon'), 13), (('granville', 'sharp'), 13), (('man', 'may'), 12), (('one', 'first'), 12)]\n","[(('savings', 'banks'), 67), (('savings', 'bank'), 67), (('working', 'classes'), 47), (('thousand', 'pounds'), 43), (('post', 'office'), 30), (('years', 'ago'), 28), (('penny', 'bank'), 28), (('working', 'men'), 27), (('working', 'man'), 27), (('per', 'cent'), 26), (('mr', 'sikes'), 25), (('hundred', 'years'), 23), (('working', 'people'), 23), (('mr', 'chadwick'), 22), (('penny', 'banks'), 22), (('old', 'age'), 21), (('office', 'savings'), 20), (('great', 'deal'), 20), (('pounds', 'year'), 20), (('shillings', 'week'), 20)]\n"]}]},{"cell_type":"code","source":["searcher = input(\"Enter search term: \").lower()\n","\n","# Filter bigrams where search term appears in either position of the tuple\n","filtered_sh_bigrams = [(k, v) for k, v in sh_bigrams.items() if searcher in k[0].lower() or searcher in k[1].lower()]\n","filtered_thrift_bigrams = [(k, v) for k, v in thrift_bigrams.items() if searcher in k[0].lower() or searcher in k[1].lower()]\n","\n","# Sort by frequency and print top 20\n","print(f\"\\nüîç Self-Help bigrams containing '{searcher}':\")\n","print(sorted(filtered_sh_bigrams, key=lambda x: -x[1])[:20])\n","\n","print(f\"\\nüîç Thrift bigrams containing '{searcher}':\")\n","print(sorted(filtered_thrift_bigrams, key=lambda x: -x[1])[:20])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jOgdETGWCGUZ","executionInfo":{"status":"ok","timestamp":1745502109627,"user_tz":-60,"elapsed":2447,"user":{"displayName":"tom compton","userId":"11608738456980900557"}},"outputId":"bea367cb-ea1e-4f28-c980-4264f503c6cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter search term: work\n","\n","üîç Self-Help bigrams containing 'work':\n","[(('set', 'work'), 17), (('hard', 'work'), 7), (('worked', 'hard'), 7), (('worked', 'way'), 6), (('working', 'man'), 6), (('working', 'trade'), 5), (('great', 'works'), 5), (('working', 'power'), 4), (('work', 'great'), 4), (('upon', 'work'), 4), (('good', 'works'), 4), (('working', 'classes'), 4), (('true', 'worker'), 3), (('must', 'work'), 3), (('worked', 'trade'), 3), (('work', 'occupied'), 3), (('willing', 'work'), 3), (('part', 'work'), 3), (('work', 'even'), 3), (('working', 'qualities'), 3)]\n","\n","üîç Thrift bigrams containing 'work':\n","[(('working', 'classes'), 47), (('working', 'men'), 27), (('working', 'man'), 27), (('working', 'people'), 23), (('hard', 'work'), 8), (('english', 'workmen'), 7), (('worked', 'way'), 6), (('work', 'must'), 5), (('condition', 'working'), 5), (('colliers', 'ironworkers'), 5), (('skilled', 'workmen'), 5), (('amongst', 'working'), 5), (('work', 'hard'), 4), (('men', 'work'), 4), (('skilled', 'workman'), 4), (('worked', 'hard'), 4), (('working', 'class'), 4), (('good', 'work'), 4), (('every', 'working'), 4), (('working', 'mans'), 4)]\n"]}]},{"cell_type":"code","source":["sh_df_bigrams = pd.DataFrame(filtered_sh_bigrams, columns = ['word','freq'])\n","thrift_df_bigrams = pd.DataFrame(filtered_thrift_bigrams, columns = ['word','freq'])\n","print(f' Self-Help top bigrams: ')\n","print(sh_df_bigrams)\n","print(f' Thrift top bigrams: ')\n","print(thrift_df_bigrams)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kWA7XRrY74rl","executionInfo":{"status":"ok","timestamp":1745502267569,"user_tz":-60,"elapsed":8,"user":{"displayName":"tom compton","userId":"11608738456980900557"}},"outputId":"a416110d-001d-46f9-c3c9-7215e3eea3a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Self-Help top bigrams: \n","                       word  freq\n","0            (true, worker)     3\n","1     (worker, stimulating)     1\n","2              (must, work)     3\n","3             (work, order)     1\n","4           (good, working)     1\n","...                     ...   ...\n","1187          (work, brief)     1\n","1188    (directed, working)     1\n","1189        (working, ship)     1\n","1190           (last, work)     1\n","1191      (work, completed)     1\n","\n","[1192 rows x 2 columns]\n"," Thrift top bigrams: \n","                                           word  freq\n","0                               (savings, work)     1\n","1                                (work, strive)     1\n","2                             (thrift, workmen)     1\n","3                      (workmen, capitalhabits)     1\n","4     (operativescolliers, ironworkersearnings)     1\n","...                                         ...   ...\n","1114                           (workmen, leads)     1\n","1115                       (workmen, tottenham)     1\n","1116                         (work, washington)     1\n","1117                      (cooking, workingmen)     1\n","1118                   (workingmen, definition)     1\n","\n","[1119 rows x 2 columns]\n"]}]},{"cell_type":"code","source":["sh_trigrams = nltk.FreqDist(nltk.trigrams(sh_tokens))\n","thrift_trigrams = nltk.FreqDist(nltk.trigrams(thrift_tokens))\n","print(sh_trigrams.most_common(20))\n","print(thrift_trigrams.most_common(20))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y9U3MWRi74v8","executionInfo":{"status":"ok","timestamp":1745502290899,"user_tz":-60,"elapsed":160,"user":{"displayName":"tom compton","userId":"11608738456980900557"}},"outputId":"71c9ffae-df82-4199-94b6-a52496c892e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(('sir', 'walter', 'scott'), 10), (('sir', 'robert', 'peel'), 9), (('sir', 'joshua', 'reynolds'), 9), (('sir', 'humphry', 'davy'), 5), (('late', 'sir', 'robert'), 4), (('twenty', 'years', 'age'), 4), (('illustrations', 'power', 'perseverance'), 4), (('sir', 'charles', 'bell'), 4), (('sir', 'charles', 'napier'), 4), (('greater', 'less', 'degree'), 3), (('sir', 'john', 'sinclair'), 3), (('morning', 'till', 'nine'), 3), (('sir', 'samuel', 'romilly'), 3), (('conservatoire', 'des', 'arts'), 3), (('des', 'arts', 'et'), 3), (('arts', 'et', 'm√©tiers'), 3), (('never', 'lost', 'sight'), 3), (('great', 'secret', 'success'), 3), (('east', 'india', 'company'), 3), (('remarkable', 'illustrations', 'power'), 3)]\n","[(('post', 'office', 'savings'), 19), (('hundred', 'years', 'ago'), 17), (('office', 'savings', 'banks'), 16), (('middle', 'upper', 'classes'), 7), (('hundred', 'thousand', 'pounds'), 7), (('_a', 'penny', 'day_'), 7), (('paris', 'universal', 'exhibition'), 6), (('five', 'per', 'cent'), 6), (('ten', 'per', 'cent'), 6), (('sir', 'francis', 'crossley'), 6), (('live', 'beyond', 'means'), 5), (('fifty', 'pounds', 'year'), 5), (('five', 'hundred', 'pounds'), 5), (('money', 'savings', 'banks'), 5), (('may', 'secure', 'sum'), 5), (('yorkshire', 'penny', 'bank'), 5), (('man', 'couldnt', 'say'), 4), (('provide', 'evil', 'day'), 4), (('men', 'women', 'children'), 4), (('hundred', 'fifty', 'pounds'), 4)]\n"]}]},{"cell_type":"code","source":["searcher = input(\"Enter search term: \").lower()\n","\n","# Filter trigrams where the search term appears in any of the three words\n","filtered_sh_trigrams = [(k, v) for k, v in sh_trigrams.items() if any(searcher in word.lower() for word in k)]\n","filtered_thrift_trigrams = [(k, v) for k, v in thrift_trigrams.items() if any(searcher in word.lower() for word in k)]\n","\n","# Sort by frequency and print top 20\n","print(f\"\\nüîç Self-Help trigrams containing '{searcher}':\")\n","print(sorted(filtered_sh_trigrams, key=lambda x: -x[1])[:20])\n","\n","print(f\"\\nüîç Thrift trigrams containing '{searcher}':\")\n","print(sorted(filtered_thrift_trigrams, key=lambda x: -x[1])[:20])\n"],"metadata":{"id":"ACHnW2DB8kdH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sh_df_trigrams = pd.DataFrame(filtered_sh_trigrams, columns = ['word','freq'])\n","thrift_df_trigrams = pd.DataFrame(filtered_thrift_trigrams, columns = ['word','freq'])\n","print(f' Self-Help top trigrams: ')\n","print(sh_df_trigrams)\n","print(f' Thrift top trigrams: ')\n","print(thrift_df_trigrams)"],"metadata":{"id":"Cw6nfmut8tEA"},"execution_count":null,"outputs":[]}]}